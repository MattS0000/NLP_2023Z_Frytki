{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we preprocess records from json downloaded from here: https://data.dws.informatik.uni-mannheim.de/largescaleproductcorpus/data/v2/goldstandards/all_gs.json.gz\n",
    "Out goal is to process each record using LLM to get better representation and than create triplets and fiveplets of of similar products acording to hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the raw product data from unziped json downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Item ID: 5923646, Right Item ID: 16920267\n",
      "Left Title: null , 417772 b21 hp xeon 5130 2 0ghz dl140 g3 new wholesale price, Right Title: 417772 b21 hp xeon 5130 2 0ghz dl140 g3 , null\n",
      "Left Description: description intel xeon 5130 dl140 g3 2 00ghz 2 core 4mb 65w full processor option kitpart number s manufacturer part 417772 b21, Right Description: description intel xeon 5130 dl140 g3 2 00ghz core 4mb 65w full processor option kitpart number s manufacturer part 417772 b21\n",
      "Left Category: proliant processor, Right Category: proliant processor\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "file_path = 'all_gs.json'\n",
    "json_data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for i in file.readlines():\n",
    "        json_data.append(json.loads(i))\n",
    "\n",
    "for item in json_data:\n",
    "    # Access individual items in each JSON object\n",
    "    id_left = item['id_left']\n",
    "    title_left = item['title_left']\n",
    "    description_left = item['description_left']\n",
    "\n",
    "    # Access KeyValuePairs\n",
    "    key_value_pairs_left = item['keyValuePairs_left']\n",
    "    category_left = key_value_pairs_left['category']\n",
    "    sub_category_left = key_value_pairs_left['sub category']\n",
    "\n",
    "    # Similarly, access fields from the right side\n",
    "    id_right = item['id_right']\n",
    "    title_right = item['title_right']\n",
    "    description_right = item['description_right']\n",
    "\n",
    "    # Access KeyValuePairs for the right side\n",
    "    key_value_pairs_right = item['keyValuePairs_right']\n",
    "    category_right = key_value_pairs_right['category']\n",
    "    sub_category_right = key_value_pairs_right['sub category']\n",
    "\n",
    "    # Additional processing or printing can be done here\n",
    "    print(f\"Left Item ID: {id_left}, Right Item ID: {id_right}\")\n",
    "    print(f\"Left Title: {title_left}, Right Title: {title_right}\")\n",
    "    print(f\"Left Description: {description_left}, Right Description: {description_right}\")\n",
    "    print(f\"Left Category: {category_left}, Right Category: {category_right}\")\n",
    "    print(\"------\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we connect to hugchat, which gives us possibility of using LLM. It is required to provide email and password for HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hugchat import hugchat\n",
    "from hugchat.login import Login\n",
    "\n",
    "def login():\n",
    "  email = \"PLACEHOLDER@gmail.com\" #EMAIL FOR HUGGING FACE\n",
    "  password = \"PLACEHOLDER\"        #PASSWORD FOR HUGGING FACE\n",
    "  sign = Login(email, password)\n",
    "  cookies = sign.login()\n",
    "  cookie_path_dir = \"./cookies_snapshot\"\n",
    "  sign.saveCookiesToDir(cookie_path_dir)\n",
    "  return hugchat.ChatBot(cookies=cookies.get_dict())\n",
    "CHATBOT = login()\n",
    "\n",
    "def query_wrapper(text):\n",
    "  id = CHATBOT.new_conversation()\n",
    "  CHATBOT.change_conversation(id)\n",
    "  return CHATBOT.query(text)\n",
    "\n",
    "def poc(similarity, title_left, description_left, title_right, description_right, prompt):\n",
    "  query_left = query_wrapper(prompt + description_left)\n",
    "  query_right = query_wrapper(prompt + description_right)\n",
    "  return similarity.similarity(title_left+str(query_left), title_right+str(query_right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a prompt that we used to create new representations for products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prompt = \"Given a product title and description, generate a meaningful text representation that captures the essence of the product for effective similarity search. Consider relevant features, attributes, and contextual information to ensure the generated representation reflects the product's unique characteristics, allowing for accurate comparisons in a similarity search algorithm. Do not answer, just create a representation.\\n\\nTEXT TO REPRESENT:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper class to save each representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Created:\n",
    "    def __init__(self,id,llm_output):\n",
    "        self.id=id\n",
    "        self.llm_output=llm_output\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function that calls LLM with given prompt+product details. After getting result it saves it to a file in /representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA=[]\n",
    "def LLM_call(i, directory):\n",
    "    CHATBOT.delete_all_conversations()\n",
    "    id = CHATBOT.new_conversation()\n",
    "    CHATBOT.change_conversation(id)\n",
    "    result=str(query_wrapper(model_prompt + i[\"title_\"] if i['title_'+directory] else \"\"+\"\\n\"+i['description_'+directory] if i['description_'+directory] else \"\"))\n",
    "    ALL_DATA.append(Created(i[\"id_\"+directory],result))\n",
    "    with open(\"representations/\"+str(i[\"id_\"+directory])+'.txt', 'w') as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we call the LLM using hugchat and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        for j,i in enumerate(json_data):\n",
    "            if i[\"id_left\"] not in ALL_DATA:\n",
    "                LLM_call(i,\"left\")\n",
    "            if i[\"id_right\"] not in ALL_DATA:\n",
    "                LLM_call(i,\"right\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a funtion to handle unicode characters that can't be used in further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_unicode(f):\n",
    "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", \"\".join(f.readlines()[2:-1]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to open generated files and return string inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_files(name):\n",
    "    with open(\"./data/representations/\"+str(name)+\".txt\",\"r\") as f:\n",
    "        return handle_unicode(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we iterate over data from original json and we connect positive pairs with negative product to create a triplet according to attribute \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = []\n",
    "base=None\n",
    "positive=None\n",
    "negative=None\n",
    "for i in json_data:\n",
    "    if i[\"label\"]==\"1\":\n",
    "        base=handle_files(i[\"id_left\"])\n",
    "        positive=handle_files(i[\"id_right\"])\n",
    "        for j in json_data:\n",
    "            if j[\"label\"]==\"0\":\n",
    "                if j[\"id_left\"]==i[\"id_left\"] or j[\"id_left\"]==i[\"id_right\"]:\n",
    "                    negative=handle_files(j[\"id_right\"])\n",
    "                    triplets.append([base,positive,negative])\n",
    "                if j[\"id_right\"]==i[\"id_left\"] or j[\"id_right\"]==i[\"id_right\"]:\n",
    "                    negative=handle_files(j[\"id_left\"])\n",
    "                    triplets.append([base,positive,negative])\n",
    "np.save(\"triplets.npy\",np.array(triplets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we iterate over data from original json. Similarly to triplets we connect positive pair with negative record. Additionaly we add a copy of a product and product from the same cluster in original data. Fiveplet will be utilized to calculate  metric proposed by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveplets = []\n",
    "base=None\n",
    "positive=None\n",
    "negative=None\n",
    "for i in json_data:\n",
    "    if i[\"label\"]==\"1\":\n",
    "        base=handle_files(i[\"id_left\"])\n",
    "        positive=handle_files(i[\"id_right\"])\n",
    "        for j in np.random.permutation(json_data):\n",
    "            if i[\"category_left\"]!=j[\"category_left\"]:\n",
    "                category = handle_files(j[\"id_left\"])\n",
    "                break\n",
    "        for j in json_data:\n",
    "            if j[\"label\"]==\"0\":\n",
    "                if j[\"id_left\"]==i[\"id_left\"] or j[\"id_left\"]==i[\"id_right\"]:\n",
    "                    negative=handle_files(j[\"id_right\"])\n",
    "                    fiveplets.append([base,base,positive,negative,category])\n",
    "                if j[\"id_right\"]==i[\"id_left\"] or j[\"id_right\"]==i[\"id_right\"]:\n",
    "                    negative=handle_files(j[\"id_left\"])\n",
    "                    fiveplets.append([base,base,positive,negative,category])\n",
    "np.save(\"fiveplets.npy\",np.array(fiveplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"fiveplets.npy\",np.array(fiveplets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
