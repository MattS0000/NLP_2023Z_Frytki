{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurif\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "from training.pretrained_bert import PretrainedBert\n",
    "from training.pretrained_distil_bert import PretrainedDistilBert\n",
    "from training.pretrained_roberta import PretrainedRoberta\n",
    "from tests.similarity import item_comparison\n",
    "from comparison.product_comparator import ProductComparator\n",
    "from preprocessing.dataset_loader import PletsDataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from evaluation.hierarchial_metric import HierachialMetric\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from training.trainer import Trainer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of raw embedding comparison\n",
    "Results of running the product comparison algorithm on abstract test.   \n",
    "This part only uses the last two steps of the pipeline, that is embedding generation (in this case, based on a pretrained model without further finetuning) and embedding similarity calculation.\n",
    "Both the value of similarity metric and the execution time will be measured. Three different models will be used to compare those values between them - Bert, DistilBert, and RoBERTa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Green apple              #\n",
      "############################\n",
      "# Similarity: 0.99615      #\n",
      "# Execution time: 36.74ms  #\n",
      "############################\n",
      "\n",
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Lemon                    #\n",
      "############################\n",
      "# Similarity: 0.98853      #\n",
      "# Execution time: 36.37ms  #\n",
      "############################\n",
      "\n",
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Brick                    #\n",
      "############################\n",
      "# Similarity: 0.98222      #\n",
      "# Execution time: 36.72ms  #\n",
      "############################\n",
      "\n",
      "############ Comparison ############\n",
      "# Red apple                        #\n",
      "# Warsaw University of Technology  #\n",
      "####################################\n",
      "# Similarity: 0.96787              #\n",
      "# Execution time: 36.10ms          #\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "model = ProductComparator(PretrainedBert().train())\n",
    "item_comparison(model, \"Red apple\", \"Green apple\")\n",
    "item_comparison(model, \"Red apple\", \"Lemon\")\n",
    "item_comparison(model, \"Red apple\", \"Brick\")\n",
    "item_comparison(model, \"Red apple\", \"Warsaw University of Technology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Green apple              #\n",
      "############################\n",
      "# Similarity: 0.99838      #\n",
      "# Execution time: 18.22ms  #\n",
      "############################\n",
      "\n",
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Lemon                    #\n",
      "############################\n",
      "# Similarity: 0.99120      #\n",
      "# Execution time: 17.93ms  #\n",
      "############################\n",
      "\n",
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Brick                    #\n",
      "############################\n",
      "# Similarity: 0.99088      #\n",
      "# Execution time: 17.83ms  #\n",
      "############################\n",
      "\n",
      "############ Comparison ############\n",
      "# Red apple                        #\n",
      "# Warsaw University of Technology  #\n",
      "####################################\n",
      "# Similarity: 0.97690              #\n",
      "# Execution time: 18.35ms          #\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "model = ProductComparator(PretrainedDistilBert().train())\n",
    "item_comparison(model, \"Red apple\", \"Green apple\")\n",
    "item_comparison(model, \"Red apple\", \"Lemon\")\n",
    "item_comparison(model, \"Red apple\", \"Brick\")\n",
    "item_comparison(model, \"Red apple\", \"Warsaw University of Technology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 481/481 [00:00<?, ?B/s] \n",
      "C:\\Users\\Aurif\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aurif\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 499M/499M [00:14<00:00, 33.5MB/s] \n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.05MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.38MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Green apple              #\n",
      "############################\n",
      "# Similarity: 0.99992      #\n",
      "# Execution time: 35.76ms  #\n",
      "############################\n",
      "\n",
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Lemon                    #\n",
      "############################\n",
      "# Similarity: 0.99961      #\n",
      "# Execution time: 35.76ms  #\n",
      "############################\n",
      "\n",
      "######## Comparison ########\n",
      "# Red apple                #\n",
      "# Brick                    #\n",
      "############################\n",
      "# Similarity: 0.99960      #\n",
      "# Execution time: 35.59ms  #\n",
      "############################\n",
      "\n",
      "############ Comparison ############\n",
      "# Red apple                        #\n",
      "# Warsaw University of Technology  #\n",
      "####################################\n",
      "# Similarity: 0.99949              #\n",
      "# Execution time: 37.63ms          #\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "model = ProductComparator(PretrainedRoberta().train())\n",
    "item_comparison(model, \"Red apple\", \"Green apple\")\n",
    "item_comparison(model, \"Red apple\", \"Lemon\")\n",
    "item_comparison(model, \"Red apple\", \"Brick\")\n",
    "item_comparison(model, \"Red apple\", \"Warsaw University of Technology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in all three models the similarities follow expected values - it is higher when compared items are close to each other (like `red apple` and `green apple`), and lower when items are different (like `red apple` and `Warsaw University of Technology`). All values are really high, reaching even 0.9949 for the most different pair of items. This is consistent with results usually present when using cosine distance between embeddings as similarity metrics. As we mostly care about the order of the similarities and not the absolute values themselves, this is not going to be a significant problem. A normalization of values with a non-linear scale can be later applied in the pipeline to make the values more intuitive to interpret.\n",
    "\n",
    "What's also worth noting are the execution times of each of the models. Both BERT and RoBERTa achieved about 35 milliseconds per pair, while DistilBERT achieved significantly better result, that being 18 milliseconds per pair. This time can be additionally improved by running the pipeline on the GPU, which we were unfortunately unable to do due to technical difficulties.\n",
    "\n",
    "Overall, the results above show that a cosine metric on BERT embeddings can be used as a good multi-hierarchical measure, when if done on an only pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of feature extraction\n",
    "Results of the feature extraction based on product title and description. The following code loads preprocessed data from a locally stored cache and displays few example triplets from the training dataset. The feature extraction is done through the use of a large language model, through an API hosted on hugging face, with the use of a single prompt per item. The training dataset consists of triples with base, a similar item, and a differing item, a per the triplet-loss implementation.\n",
    "\n",
    "We consider feature extraction as a data preprocessing step, therefore its execution time is not counted toward the execution time of the pipeline (as a preprocessed dataset can be generated once and stored in memory for all future comparisons). As such, execution time of feature extraction was not measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BASE]\n",
      "\"Intel Xeon 5130 DL140 G3 2.0GHz Quad-Core Processor - 4MB Cache, 65W TDP, Full Processor Option Kit - Part Number: 417772 B21\"\n",
      "\n",
      "This representation includes the following relevant features and attributes:\n",
      "\n",
      "* Brand: Intel\n",
      "* Model: Xeon 5130\n",
      "* Speed: 2.0GHz\n",
      "* Cores: Quad-core\n",
      "* Cache: 4MB\n",
      "* Thermal Design Power (TDP): 65W\n",
      "* Processor Type: Full Processor Option Kit\n",
      "* Part Number: 417772 B21\n",
      "\n",
      "[SIMILAR]\n",
      "\"Intel Xeon 5130 2GHz 4MB L2 Processor - HP\n",
      "\n",
      "* Intel Xeon 5000 sequence\n",
      "* 2 GHz processor frequency\n",
      "* LGA 771 socket J\n",
      "* 65nm 64-bit technology\n",
      "* 291M transistors\n",
      "* 143mm processing die size\n",
      "* Thermal Design Power (TDP): 65W\n",
      "* Supports Intel Virtualization Technology (VT)\n",
      "* Enhanced Intel SpeedStep Technology\n",
      "* Execute Disable Bit\n",
      "* Idle States: C0, C1, C2...\n",
      "* On-die digital thermal sensor\n",
      "* Protective thermal management features\n",
      "\n",
      "[DIFFERENT]\n",
      "\"Intel Xeon L5335 DL140 G3 2.0GHz 4-Core 8MB 80W Full Processor Option Kit - Part Number: 453475 B21\"\n",
      "\n",
      "This representation includes the following relevant features and attributes:\n",
      "\n",
      "* Brand: Intel\n",
      "* Model: Xeon L5335\n",
      "* Speed: 2.0GHz\n",
      "* Cores: 4\n",
      "* Memory: 8MB\n",
      "* Power Consumption: 80W\n",
      "* Processor Type: Full Processor Option Kit\n",
      "* Part Number: 453475 B21\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "[BASE]\n",
      "\"Intel Xeon E5506 DL ML370 G6 2.13GHz 4-Core 4MB 80W DDR3 Full Processor Option Kit - Part Number: 495942 B21\"\n",
      "\n",
      "This representation includes the following relevant features and attributes:\n",
      "\n",
      "* Intel Xeon E5506: The specific model of the processor\n",
      "* DL ML370 G6: The motherboard chipset and form factor (DL = dual-processor capable, ML370 = Mobile PCI Express Module, G6 = Generation 6)\n",
      "* 2.13GHz: The clock speed of the processor\n",
      "* 4-Core: The number of processing cores\n",
      "* 4MB: The size of the processor's cache memory\n",
      "* 80W: The maximum power consumption of the processor\n",
      "* DDR3 Full: The type of memory supported by the processor (DDR3) and the fact that it supports full-speed operation\n",
      "* Processor Option Kit: A bundle of components that includes the processor, heat sink, and fan\n",
      "* Part Number: A unique identifier for the product, which can be used for inventory management, ordering, and customer support purposes\n",
      "\n",
      "[SIMILAR]\n",
      "\"Intel Xeon E5506 DL ML370 G6 2.13GHz 4C 4MB 80W DDR3 Full Processor Option Kit - Part Number: 495942 B21\"\n",
      "\n",
      "This representation includes the following features and attributes:\n",
      "\n",
      "* Brand: Intel\n",
      "* Model: Xeon E5506\n",
      "* Speed: 2.13GHz\n",
      "* Cores: 4\n",
      "* Memory: 4MB\n",
      "* Power Consumption: 80W\n",
      "* Processor Type: DDR3 Full\n",
      "* Product Line: ML370 G6\n",
      "* Part Number: 495942 B21\n",
      "\n",
      "[DIFFERENT]\n",
      "\"HP Intel Xeon E5506 DL380 G6 2.13GHz 4-Core 4MB 80W Full Processor Option Kit - Part Number: 492131 B21\"\n",
      "\n",
      "####################################################################################################\n",
      "\n",
      "[BASE]\n",
      "\"Intel Xeon E5506 DL ML370 G6 2.13GHz 4-Core 4MB 80W DDR3 Full Processor Option Kit - Part Number: 495942 B21\"\n",
      "\n",
      "This representation includes the following relevant features and attributes:\n",
      "\n",
      "* Intel Xeon E5506: The specific model of the processor\n",
      "* DL ML370 G6: The motherboard chipset and form factor (DL = dual-processor capable, ML370 = Mobile PCI Express Module, G6 = Generation 6)\n",
      "* 2.13GHz: The clock speed of the processor\n",
      "* 4-Core: The number of processing cores\n",
      "* 4MB: The size of the processor's cache memory\n",
      "* 80W: The maximum power consumption of the processor\n",
      "* DDR3 Full: The type of memory supported by the processor (DDR3) and the fact that it supports full-speed operation\n",
      "* Processor Option Kit: A bundle of components that includes the processor, heat sink, and fan\n",
      "* Part Number: A unique identifier for the product, which can be used for inventory management, ordering, and customer support purposes\n",
      "\n",
      "[SIMILAR]\n",
      "\"Intel Xeon E5506 DL ML370 G6 2.13GHz 4C 4MB 80W DDR3 Full Processor Option Kit - Part Number: 495942 B21\"\n",
      "\n",
      "This representation includes the following features and attributes:\n",
      "\n",
      "* Brand: Intel\n",
      "* Model: Xeon E5506\n",
      "* Speed: 2.13GHz\n",
      "* Cores: 4\n",
      "* Memory: 4MB\n",
      "* Power Consumption: 80W\n",
      "* Processor Type: DDR3 Full\n",
      "* Product Line: ML370 G6\n",
      "* Part Number: 495942 B21\n",
      "\n",
      "[DIFFERENT]\n",
      "\"356820 CT1 DL585 4P OPT 848 2 2GHz Wholesale Price\"\n",
      "\n",
      "This representation includes the following features and attributes:\n",
      "\n",
      "* Product code: 356820\n",
      "* Type: CT1\n",
      "* Model: DL585\n",
      "* Processor: 4P\n",
      "* Operating frequency: 2GHz\n",
      "* Memory: 848\n",
      "* Number of cores: 2\n",
      "* Price: Wholesale\n"
     ]
    }
   ],
   "source": [
    "dataset = DataLoader(PletsDataset(\"./../data\", \"train_preprocessed\"), batch_size=5)\n",
    "for c, i in enumerate(dataset):\n",
    "    example = np.array(i).transpose()[0]\n",
    "    print(\"[BASE]\")\n",
    "    print(example[0])\n",
    "    print(\"\\n[SIMILAR]\")\n",
    "    print(example[1])\n",
    "    print(\"\\n[DIFFERENT]\")\n",
    "    print(example[2])\n",
    "    if c == 2:\n",
    "        break\n",
    "    print(\"\\n\"+\"#\"*100+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the feature extraction working correctly - each item is converted into a list-like string of attributes and their values. In most cases, attributes names match between items, which is highly desirable. We can also confirm that the triplets are constructed in the proper way - the similar item is indeed the same item as the base, and the different item is not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric demonstration\n",
    "The section below presents the novel metric introduced by us to measure the quality of a multi-hierarchical similarity measure. This is done by calculating metric value for a artificial examples of similarity values between a base item and a set of other items, with the first one being the most similar, and the last one being the most different. For example, `[1, 0.75, 0.5, 0.25, 0]` means that the similarity between two identical items is `1`, between two somewhat similar items is `0.5`, and between completely different items is `0`. This is the desirable distribution of similarities, which should achieve the highest values of the metric.\n",
    "\n",
    "The goal of the metric is to measure how well the given similarities match the expect result. This consists of two factors - proper ordering of similarities (from largest to smallest) and even spacing between values (having a big difference in similarity scores between exact items and between different items is more desirable than for this difference being small, even if in proper order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise similarities: [1, 0.75, 0.5, 0.25, 0], metric value: 1.0\n",
      "Pairwise similarities: [1, 0.95, 0.9, 0.85, 0.8], metric value: 0.88\n",
      "Pairwise similarities: [1, 0.999, 0.998, 0.997, 0.996], metric value: 0.813997\n",
      "Pairwise similarities: [1, 1, 1, 1, 1], metric value: 0.3125\n",
      "Pairwise similarities: [1, 0.75, 0.9, 0.25, 0], metric value: 0.9339999999999999\n",
      "Pairwise similarities: [0, 0.25, 0.5, 0.75, 1], metric value: 0.25\n",
      "Pairwise similarities: [1, 0, 1, 0, 1], metric value: 0.4625\n",
      "Pairwise similarities: [0, 0, 1, 0, 1], metric value: 0.26250000000000007\n",
      "Pairwise similarities: [1, 0, 0, 0, 0], metric value: 0.6125\n",
      "Pairwise similarities: [0, 1, 0, 0, 0], metric value: 0.5125\n",
      "Pairwise similarities: [0, 0, 0, 0, 1], metric value: 0.21250000000000005\n"
     ]
    }
   ],
   "source": [
    "presets = [\n",
    "    [1, 0.75, 0.5, 0.25, 0],\n",
    "    [1, 0.95, 0.9, 0.85, 0.8],\n",
    "    [1, 0.999, 0.998, 0.997, 0.996],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0.75, 0.9, 0.25, 0],\n",
    "    [0, 0.25, 0.5, 0.75, 1],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1]\n",
    "]\n",
    "metric = HierachialMetric()\n",
    "for preset in presets:\n",
    "    print(f\"Pairwise similarities: {preset}, metric value: {metric.evaluate_similarities(preset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the metric designed by us has all of the desired properties. It achieves the value of `1` for the perfect distribution (`[1, 0.75, 0.5, 0.25, 0]`), and tha value drops once the difference between similarities gets smaller. Having a single mismatch item while other still being optimal still returns a high value. Distributions that provide no information (such as `[1, 1, 1, 1, 1]` and `[1, 0, 0, 0, 0]`) have low scores. Having similarities differ from the truth (more similar to a different item than to the exact one) achieves even lower values. It is also impossible to achieve score of `0` with this metric, however this is not a major concern."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "The code belows fine-tunes the pretrained models with the triplets from the training dataset. This is done for the Bert, DistilBert, and RoBERTa models. All of the trained models are stored in a pickle format to allow later access without needing to retrain the models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurif\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/5371 (0%)]\tLoss: 11.185242\n",
      "Train: [100/5371 (2%)]\tLoss: 5.731047\n",
      "Train: [200/5371 (4%)]\tLoss: 6.335767\n",
      "Train: [300/5371 (6%)]\tLoss: 5.671490\n",
      "Train: [400/5371 (7%)]\tLoss: 5.770305\n",
      "Train: [500/5371 (9%)]\tLoss: 4.061690\n",
      "Train: [600/5371 (11%)]\tLoss: 5.902798\n",
      "Train: [700/5371 (13%)]\tLoss: 6.661523\n",
      "Train: [800/5371 (15%)]\tLoss: 4.379632\n",
      "Train: [900/5371 (17%)]\tLoss: 4.784111\n",
      "Train: [1000/5371 (19%)]\tLoss: 3.513095\n",
      "Train: [1100/5371 (20%)]\tLoss: 3.954347\n",
      "Train: [1200/5371 (22%)]\tLoss: 3.570581\n",
      "Train: [1300/5371 (24%)]\tLoss: 3.821434\n",
      "Train: [1400/5371 (26%)]\tLoss: 3.683206\n",
      "Train: [1500/5371 (28%)]\tLoss: 3.784428\n",
      "Epoch: 1/3. Train set: Average loss: 4.7146\n",
      "Train: [0/5371 (0%)]\tLoss: 10.240974\n",
      "Train: [100/5371 (2%)]\tLoss: 5.079950\n",
      "Train: [200/5371 (4%)]\tLoss: 4.276255\n",
      "Train: [300/5371 (6%)]\tLoss: 3.653752\n",
      "Train: [400/5371 (7%)]\tLoss: 4.260196\n",
      "Train: [500/5371 (9%)]\tLoss: 3.517990\n",
      "Train: [600/5371 (11%)]\tLoss: 3.381110\n",
      "Train: [700/5371 (13%)]\tLoss: 4.265030\n",
      "Train: [800/5371 (15%)]\tLoss: 3.599481\n",
      "Train: [900/5371 (17%)]\tLoss: 3.599495\n",
      "Train: [1000/5371 (19%)]\tLoss: 4.079455\n",
      "Train: [1100/5371 (20%)]\tLoss: 3.731596\n",
      "Train: [1200/5371 (22%)]\tLoss: 3.476178\n",
      "Train: [1300/5371 (24%)]\tLoss: 3.880966\n",
      "Train: [1400/5371 (26%)]\tLoss: 2.807259\n",
      "Train: [1500/5371 (28%)]\tLoss: 3.702574\n",
      "Epoch: 2/3. Train set: Average loss: 3.8055\n",
      "Train: [0/5371 (0%)]\tLoss: 0.981731\n",
      "Train: [100/5371 (2%)]\tLoss: 3.316989\n",
      "Train: [200/5371 (4%)]\tLoss: 3.730943\n",
      "Train: [300/5371 (6%)]\tLoss: 2.992091\n",
      "Train: [400/5371 (7%)]\tLoss: 2.412807\n",
      "Train: [500/5371 (9%)]\tLoss: 2.975975\n",
      "Train: [600/5371 (11%)]\tLoss: 3.884052\n",
      "Train: [700/5371 (13%)]\tLoss: 3.211392\n",
      "Train: [800/5371 (15%)]\tLoss: 3.721626\n",
      "Train: [900/5371 (17%)]\tLoss: 2.514788\n",
      "Train: [1000/5371 (19%)]\tLoss: 2.484637\n",
      "Train: [1100/5371 (20%)]\tLoss: 2.721646\n",
      "Train: [1200/5371 (22%)]\tLoss: 3.478154\n",
      "Train: [1300/5371 (24%)]\tLoss: 3.215982\n",
      "Train: [1400/5371 (26%)]\tLoss: 2.672068\n",
      "Train: [1500/5371 (28%)]\tLoss: 2.403471\n",
      "Epoch: 3/3. Train set: Average loss: 3.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurif\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/5371 (0%)]\tLoss: 10.067198\n",
      "Train: [100/5371 (2%)]\tLoss: 5.584477\n",
      "Train: [200/5371 (4%)]\tLoss: 5.765273\n",
      "Train: [300/5371 (6%)]\tLoss: 5.140297\n",
      "Train: [400/5371 (7%)]\tLoss: 5.321143\n",
      "Train: [500/5371 (9%)]\tLoss: 5.584893\n",
      "Train: [600/5371 (11%)]\tLoss: 5.545220\n",
      "Train: [700/5371 (13%)]\tLoss: 4.971832\n",
      "Train: [800/5371 (15%)]\tLoss: 4.146455\n",
      "Train: [900/5371 (17%)]\tLoss: 3.158454\n",
      "Train: [1000/5371 (19%)]\tLoss: 3.796669\n",
      "Train: [1100/5371 (20%)]\tLoss: 4.180141\n",
      "Train: [1200/5371 (22%)]\tLoss: 5.147162\n",
      "Train: [1300/5371 (24%)]\tLoss: 4.517372\n",
      "Train: [1400/5371 (26%)]\tLoss: 3.306258\n",
      "Train: [1500/5371 (28%)]\tLoss: 3.812942\n",
      "Epoch: 1/3. Train set: Average loss: 4.6551\n",
      "Train: [0/5371 (0%)]\tLoss: 0.000000\n",
      "Train: [100/5371 (2%)]\tLoss: 4.042134\n",
      "Train: [200/5371 (4%)]\tLoss: 4.090084\n",
      "Train: [300/5371 (6%)]\tLoss: 2.679219\n",
      "Train: [400/5371 (7%)]\tLoss: 3.605400\n",
      "Train: [500/5371 (9%)]\tLoss: 3.805888\n",
      "Train: [600/5371 (11%)]\tLoss: 2.995001\n",
      "Train: [700/5371 (13%)]\tLoss: 2.295758\n",
      "Train: [800/5371 (15%)]\tLoss: 2.945246\n",
      "Train: [900/5371 (17%)]\tLoss: 3.287746\n",
      "Train: [1000/5371 (19%)]\tLoss: 3.084664\n",
      "Train: [1100/5371 (20%)]\tLoss: 2.829180\n",
      "Train: [1200/5371 (22%)]\tLoss: 2.633538\n",
      "Train: [1300/5371 (24%)]\tLoss: 2.930592\n",
      "Train: [1400/5371 (26%)]\tLoss: 2.482000\n",
      "Train: [1500/5371 (28%)]\tLoss: 3.255796\n",
      "Epoch: 2/3. Train set: Average loss: 3.1130\n",
      "Train: [0/5371 (0%)]\tLoss: 0.000000\n",
      "Train: [100/5371 (2%)]\tLoss: 1.921426\n",
      "Train: [200/5371 (4%)]\tLoss: 3.293818\n",
      "Train: [300/5371 (6%)]\tLoss: 2.411950\n",
      "Train: [400/5371 (7%)]\tLoss: 2.510633\n",
      "Train: [500/5371 (9%)]\tLoss: 2.791015\n",
      "Train: [600/5371 (11%)]\tLoss: 2.568898\n",
      "Train: [700/5371 (13%)]\tLoss: 2.963729\n",
      "Train: [800/5371 (15%)]\tLoss: 2.176695\n",
      "Train: [900/5371 (17%)]\tLoss: 2.764268\n",
      "Train: [1000/5371 (19%)]\tLoss: 3.274770\n",
      "Train: [1100/5371 (20%)]\tLoss: 2.759739\n",
      "Train: [1200/5371 (22%)]\tLoss: 2.398104\n",
      "Train: [1300/5371 (24%)]\tLoss: 2.534935\n",
      "Train: [1400/5371 (26%)]\tLoss: 2.714747\n",
      "Train: [1500/5371 (28%)]\tLoss: 2.541675\n",
      "Epoch: 3/3. Train set: Average loss: 2.6496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/5371 (0%)]\tLoss: 0.000000\n",
      "Train: [100/5371 (2%)]\tLoss: 14.559117\n",
      "Train: [200/5371 (4%)]\tLoss: 6.791852\n",
      "Train: [300/5371 (6%)]\tLoss: 6.399161\n",
      "Train: [400/5371 (7%)]\tLoss: 7.312668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m train_model(PretrainedBert(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     13\u001B[0m train_model(PretrainedDistilBert(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistilbert_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPretrainedRoberta\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroberta_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(base, label)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model\u001B[39m(base, label):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m         model \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m         pickle\u001B[38;5;241m.\u001B[39mdump(model, \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./../models/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32md:\\PW\\NLP\\NLP_2023Z_Frytki\\P1_Final\\code\\training\\trainer.py:43\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, train_data, epochs)\u001B[0m\n\u001B[0;32m     41\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m lr_scheduler\u001B[38;5;241m.\u001B[39mStepLR(optimizer, \u001B[38;5;241m8\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, last_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     42\u001B[0m log_interval \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m---> 43\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32md:\\PW\\NLP\\NLP_2023Z_Frytki\\P1_Final\\code\\siamese_triplet\\trainer.py:23\u001B[0m, in \u001B[0;36mfit\u001B[1;34m(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics, start_epoch)\u001B[0m\n\u001B[0;32m     20\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Train stage\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m train_loss, metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m message \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. Train set: Average loss: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, n_epochs, train_loss)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric \u001B[38;5;129;01min\u001B[39;00m metrics:\n",
      "File \u001B[1;32md:\\PW\\NLP\\NLP_2023Z_Frytki\\P1_Final\\code\\siamese_triplet\\trainer.py:53\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\u001B[0m\n\u001B[0;32m     49\u001B[0m         target \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m     52\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 53\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(outputs) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     56\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (outputs,)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\PW\\NLP\\NLP_2023Z_Frytki\\P1_Final\\code\\siamese_triplet\\networks.py:83\u001B[0m, in \u001B[0;36mTripletNet.forward\u001B[1;34m(self, x1, x2, x3)\u001B[0m\n\u001B[0;32m     81\u001B[0m output1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_net(x1)\n\u001B[0;32m     82\u001B[0m output2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_net(x2)\n\u001B[1;32m---> 83\u001B[0m output3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx3\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output1, output2, output3\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\PW\\NLP\\NLP_2023Z_Frytki\\P1_Final\\code\\training\\embedding_net.py:13\u001B[0m, in \u001B[0;36mEmbeddingNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     12\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(x, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 13\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moutput)\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlast_hidden_state\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    826\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    828\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    829\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    830\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    833\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    834\u001B[0m )\n\u001B[1;32m--> 835\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    836\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    847\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    848\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    513\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    514\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    515\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    521\u001B[0m         output_attentions,\n\u001B[0;32m    522\u001B[0m     )\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    452\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    453\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[1;32m--> 455\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    458\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[0;32m    460\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pytorch_utils.py:241\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    238\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[0;32m    239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[1;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:468\u001B[0m, in \u001B[0;36mRobertaLayer.feed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[0;32m    467\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate(attention_output)\n\u001B[1;32m--> 468\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mintermediate_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:380\u001B[0m, in \u001B[0;36mRobertaOutput.forward\u001B[1;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[0;32m    378\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor, input_tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    379\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(hidden_states)\n\u001B[1;32m--> 380\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    381\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_states \u001B[38;5;241m+\u001B[39m input_tensor)\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001B[0m, in \u001B[0;36mDropout.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:1252\u001B[0m, in \u001B[0;36mdropout\u001B[1;34m(input, p, training, inplace)\u001B[0m\n\u001B[0;32m   1250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[0;32m   1251\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout probability has to be between 0 and 1, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p))\n\u001B[1;32m-> 1252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mdropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = DataLoader(PletsDataset(\"./../data\", \"train_preprocessed\"), batch_size=1)\n",
    "def train_model(base, label):\n",
    "    try:\n",
    "        model = Trainer(base).train(train_dataset, epochs=3)\n",
    "        pickle.dump(model, open(f\"./../models/{label}\", \"wb\"))\n",
    "    except Exception as e:\n",
    "        if e.args[0] == \"KeyboardInterrupt\":\n",
    "            raise e\n",
    "        else:\n",
    "            print(e)\n",
    "\n",
    "train_model(PretrainedBert(), \"bert_model\")\n",
    "train_model(PretrainedDistilBert(), \"distilbert_model\")\n",
    "train_model(PretrainedRoberta(), \"roberta_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation study - model architecture, training procedure\n",
    "Here we compare both the raw pretrained models and the fine-tuned ones for each of the architectures. This is done through both the execution time measured previously in this notebook, and through calculating the values of our multi-hierarchical metric on a subset of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:00<00:07,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9811779260635376, 0.9956138134002686, 0.9569826722145081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:00<00:06,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9811779260635376, 0.7254902720451355, 0.9569826722145081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:01<00:05,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9811779260635376, 0.9975031614303589, 0.9569826722145081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:01<00:04,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9811779260635376, 0.9750349521636963, 0.9569826722145081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:02<00:05,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.686486542224884, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:03<00:05,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.9513614177703857, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:03<00:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.9499788284301758, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:04<00:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.6608355641365051, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:05<00:04,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.943268895149231, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:06<00:03,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.6813088655471802, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:06<00:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9597380757331848, 0.9527252316474915, 0.9481768012046814]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:07<00:01,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9768575429916382, 0.6272746324539185, 0.9772341847419739]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:07<00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9768575429916382, 0.9885523319244385, 0.9772341847419739]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:08<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9768575429916382, 0.9840190410614014, 0.9772341847419739]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:08<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.9768575429916382, 0.9808449149131775, 0.9772341847419739]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:00<00:03,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9476534128189087, 0.9538512825965881, 0.9508057832717896, 0.9564324617385864]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:00<00:03,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9537837505340576, 0.9514871835708618, 0.9492273926734924, 0.956266462802887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:00<00:02,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9445162415504456, 0.9553468823432922, 0.9604204893112183, 0.9506247043609619]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:00<00:02,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94693922996521, 0.9507322311401367, 0.9532399773597717, 0.9419180154800415]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:01<00:03,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9483597278594971, 0.9486910104751587, 0.949203610420227, 0.9542354345321655]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:01<00:03,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9560816287994385, 0.945728600025177, 0.95486980676651, 0.9510737657546997]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:02<00:02,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9412915110588074, 0.9441735744476318, 0.9487171173095703, 0.9512507915496826]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:02<00:02,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.961908757686615, 0.9445012807846069, 0.9419769048690796, 0.9491645693778992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:03<00:02,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9427945613861084, 0.9539462924003601, 0.957696795463562, 0.9492045640945435]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:03<00:02,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9530354738235474, 0.9627180099487305, 0.9474023580551147, 0.9494526386260986]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:03<00:01,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9417166709899902, 0.9568955302238464, 0.9497734904289246, 0.9528235197067261]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:04<00:01,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9501188397407532, 0.9475934505462646, 0.9444085359573364, 0.9511353969573975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:04<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9418435096740723, 0.9484652280807495, 0.9493737816810608, 0.955552339553833]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:04<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9544762969017029, 0.9520158171653748, 0.9352498054504395, 0.9564002156257629]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:04<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9401679039001465, 0.9582657217979431, 0.9578885436058044, 0.951780378818512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Raw BERT\": PretrainedBert().train(),\n",
    "    \"Raw DistilBERT\": PretrainedDistilBert().train(),\n",
    "    \"Raw RoBERTa\": PretrainedRoberta().train(),\n",
    "    \"Trained BERT\": pickle.load(open(\"./../models/bert_model\", \"rb\")),\n",
    "    \"Trained DistilBERT\": pickle.load(open(\"./../models/distilbert_model\", \"rb\")),\n",
    "    # \"Trained RoBERTa\": pickle.load(open(\"./../models/roberta_model\", \"rb\"))\n",
    "}\n",
    "test_dataset = DataLoader(PletsDataset(\"./../data\", \"test_preprocessed\", direct=True))\n",
    "metric = HierachialMetric()\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model\", \"score\"])\n",
    "for label, model in models.items():\n",
    "    comparator = ProductComparator(model)\n",
    "    score = metric.evaluate_dataset(comparator, test_dataset, limit=150)\n",
    "    df.loc[len(df)] = [label, score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model     score\n",
      "0            Raw BERT  0.772943\n",
      "1      Raw DistilBERT  0.762488\n",
      "2         Raw RoBERTa  0.753059\n",
      "3        Trained BERT  0.581942\n",
      "4  Trained DistilBERT  0.561562\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAINCAYAAAAJGy/3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6qElEQVR4nO3dfZhVZb038O/MCIO8KqIzSJxQMYNSUBBCM60m8ejx0UrjmAlyDCslNdKjmIHvY6mIFUqZaObpSCVPVhq+kFgpTxhkppFmpZAxvIiCYkHN7OcPLydHBhcizgbn87mudV3se9/32r81cw97f/da+94VpVKpFAAAADaqstwFAAAAbO0EJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACmxX7gLaWlNTU/7617+mW7duqaioKHc5AABAmZRKpTz//PPZddddU1n52ueU2l1w+utf/5q+ffuWuwwAAGArsWTJkrztbW97zT7tLjh169YtyUs/nO7du5e5GgAAoFzWrFmTvn37NmeE11L24DRt2rRcfvnlaWhoyKBBg/LVr341w4YN22j/qVOn5tprr83ixYvTq1evHHPMMamvr0+nTp026fFevjyve/fughMAALBJH+Ep6+IQM2fOzIQJEzJ58uQsXLgwgwYNysiRI7N8+fJW+3/nO9/JOeeck8mTJ2fRokW5/vrrM3PmzJx77rltXDkAANCelDU4TZkyJePGjcvYsWMzcODATJ8+PZ07d86MGTNa7f/AAw/kwAMPzMc//vH069cvhx56aI477rjMnz+/jSsHAADak7IFp/Xr12fBggWpq6v7VzGVlamrq8u8efNaHXPAAQdkwYIFzUHpT3/6U+64444cfvjhG32cdevWZc2aNS02AACA16Nsn3FauXJlGhsbU1NT06K9pqYmv//971sd8/GPfzwrV67Me9/73pRKpfzzn//Mpz/96de8VK++vj4XXHDBFq0dAABoX7apL8CdO3duLr300lxzzTVZuHBhZs2aldtvvz0XXXTRRsdMnDgxq1evbt6WLFnShhUDAABvBWU749SrV69UVVVl2bJlLdqXLVuW2traVsd88YtfzAknnJBPfvKTSZK99947a9euzcknn5wvfOELrX5pVXV1daqrq7f8AQAAAO1G2c44dezYMUOGDMmcOXOa25qamjJnzpyMGDGi1TEvvvjiBuGoqqoqyUvf+gsAAPBmKOv3OE2YMCFjxozJ0KFDM2zYsEydOjVr167N2LFjkySjR49Onz59Ul9fnyQ58sgjM2XKlOy7774ZPnx4nnjiiXzxi1/MkUce2RygAAAAtrSyBqdRo0ZlxYoVmTRpUhoaGjJ48ODMnj27ecGIxYsXtzjDdN5556WioiLnnXdenn766ey888458sgjc8kll5TrEAAAgHagotTOrnFbs2ZNevTokdWrV6d79+7lLgcAACiT15MNtqlV9QAAAMpBcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUGC7chewLRpy1k3lLoE2tODy0eUuAQCAMnPGCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAAClhVD7ZiVnBsX6zgCABbL2ecAAAACghOAAAABQQnAACAAoITAABAAYtDAGAhknbGQiQAr58zTgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAW2iuA0bdq09OvXL506dcrw4cMzf/78jfY95JBDUlFRscF2xBFHtGHFAABAe1L24DRz5sxMmDAhkydPzsKFCzNo0KCMHDkyy5cvb7X/rFmzsnTp0ubtkUceSVVVVY499tg2rhwAAGgvyh6cpkyZknHjxmXs2LEZOHBgpk+fns6dO2fGjBmt9u/Zs2dqa2ubt7vvvjudO3cWnAAAgDdNWYPT+vXrs2DBgtTV1TW3VVZWpq6uLvPmzdukfVx//fX5z//8z3Tp0qXV+9etW5c1a9a02AAAAF6PsganlStXprGxMTU1NS3aa2pq0tDQUDh+/vz5eeSRR/LJT35yo33q6+vTo0eP5q1v375vuG4AAKB9Kfulem/E9ddfn7333jvDhg3baJ+JEydm9erVzduSJUvasEIAAOCtYLtyPnivXr1SVVWVZcuWtWhftmxZamtrX3Ps2rVrc8stt+TCCy98zX7V1dWprq5+w7UCAADtV1nPOHXs2DFDhgzJnDlzmtuampoyZ86cjBgx4jXHfu9738u6devyiU984s0uEwAAaOfKesYpSSZMmJAxY8Zk6NChGTZsWKZOnZq1a9dm7NixSZLRo0enT58+qa+vbzHu+uuvz9FHH52ddtqpHGUDAADtSNmD06hRo7JixYpMmjQpDQ0NGTx4cGbPnt28YMTixYtTWdnyxNhjjz2WX/ziF7nrrrvKUTIAANDOlD04Jcn48eMzfvz4Vu+bO3fuBm177bVXSqXSm1wVAADAS7bpVfUAAADaguAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABcoenKZNm5Z+/fqlU6dOGT58eObPn/+a/Z977rmceuqp6d27d6qrq/OOd7wjd9xxRxtVCwAAtEfblfPBZ86cmQkTJmT69OkZPnx4pk6dmpEjR+axxx7LLrvsskH/9evX50Mf+lB22WWXfP/730+fPn3y1FNPZYcddmj74gEAgHajrMFpypQpGTduXMaOHZskmT59em6//fbMmDEj55xzzgb9Z8yYkVWrVuWBBx5Ihw4dkiT9+vVry5IBAIB2qGyX6q1fvz4LFixIXV3dv4qprExdXV3mzZvX6pgf/vCHGTFiRE499dTU1NTk3e9+dy699NI0NjZu9HHWrVuXNWvWtNgAAABej7IFp5UrV6axsTE1NTUt2mtqatLQ0NDqmD/96U/5/ve/n8bGxtxxxx354he/mCuvvDIXX3zxRh+nvr4+PXr0aN769u27RY8DAAB46yv74hCvR1NTU3bZZZd84xvfyJAhQzJq1Kh84QtfyPTp0zc6ZuLEiVm9enXztmTJkjasGAAAeCso22ecevXqlaqqqixbtqxF+7Jly1JbW9vqmN69e6dDhw6pqqpqbhswYEAaGhqyfv36dOzYcYMx1dXVqa6u3rLFAwAA7UrZzjh17NgxQ4YMyZw5c5rbmpqaMmfOnIwYMaLVMQceeGCeeOKJNDU1Nbc9/vjj6d27d6uhCQAAYEso66V6EyZMyHXXXZdvfetbWbRoUT7zmc9k7dq1zavsjR49OhMnTmzu/5nPfCarVq3K6aefnscffzy33357Lr300px66qnlOgQAAKAdKOty5KNGjcqKFSsyadKkNDQ0ZPDgwZk9e3bzghGLFy9OZeW/sl3fvn1z55135nOf+1z22Wef9OnTJ6effnrOPvvsch0CAADQDpQ1OCXJ+PHjM378+Fbvmzt37gZtI0aMyP/7f//vTa4KAADgX7apVfUAAADKQXACAAAoIDgBAAAUKPtnnACA9mPIWTeVuwTa0ILLR5e7BNhinHECAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAApsV+4CAABgSxty1k3lLoE2tODy0W/6YzjjBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAltFcJo2bVr69euXTp06Zfjw4Zk/f/5G+954442pqKhosXXq1KkNqwUAANqbsgenmTNnZsKECZk8eXIWLlyYQYMGZeTIkVm+fPlGx3Tv3j1Lly5t3p566qk2rBgAAGhvyh6cpkyZknHjxmXs2LEZOHBgpk+fns6dO2fGjBkbHVNRUZHa2trmraampg0rBgAA2puyBqf169dnwYIFqaura26rrKxMXV1d5s2bt9FxL7zwQt7+9renb9++Oeqoo/Loo49utO+6deuyZs2aFhsAAMDrUdbgtHLlyjQ2Nm5wxqimpiYNDQ2tjtlrr70yY8aM3Hbbbbn55pvT1NSUAw44IH/5y19a7V9fX58ePXo0b3379t3ixwEAALy1lf1SvddrxIgRGT16dAYPHpyDDz44s2bNys4775yvf/3rrfafOHFiVq9e3bwtWbKkjSsGAAC2dduV88F79eqVqqqqLFu2rEX7smXLUltbu0n76NChQ/bdd9888cQTrd5fXV2d6urqN1wrAADQfpX1jFPHjh0zZMiQzJkzp7mtqakpc+bMyYgRIzZpH42Njfntb3+b3r17v1llAgAA7VxZzzglyYQJEzJmzJgMHTo0w4YNy9SpU7N27dqMHTs2STJ69Oj06dMn9fX1SZILL7ww73nPe9K/f/8899xzufzyy/PUU0/lk5/8ZDkPAwAAeAsre3AaNWpUVqxYkUmTJqWhoSGDBw/O7NmzmxeMWLx4cSor/3Vi7Nlnn824cePS0NCQHXfcMUOGDMkDDzyQgQMHlusQAACAt7iyB6ckGT9+fMaPH9/qfXPnzm1x+6qrrspVV13VBlUBAAC8ZJtbVQ8AAKCtCU4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACiw2cHpueeeyze/+c1MnDgxq1atSpIsXLgwTz/99BYrDgAAYGuw3eYMevjhh1NXV5cePXrkySefzLhx49KzZ8/MmjUrixcvzk033bSl6wQAACibzTrjNGHChJx44on5wx/+kE6dOjW3H3744fnZz362xYoDAADYGmxWcHrwwQfzqU99aoP2Pn36pKGh4Q0XBQAAsDXZrOBUXV2dNWvWbND++OOPZ+edd37DRQEAAGxNNis4/Z//839y4YUX5h//+EeSpKKiIosXL87ZZ5+dj370o1u0QAAAgHLbrOB05ZVX5oUXXsguu+ySv/3tbzn44IPTv3//dOvWLZdccsmWrhEAAKCsNmtVvR49euTuu+/O/fffn9/85jd54YUXst9++6Wurm5L1wcAAFB2rzs4/eMf/8j222+fhx56KAceeGAOPPDAN6MuAACArcbrvlSvQ4cO+bd/+7c0Nja+GfUAAABsdTbrM05f+MIXcu6552bVqlVbuh4AAICtzmZ9xulrX/tannjiiey66655+9vfni5durS4f+HChVukOAAAgK3BZgWno48+eguXAQAAsPXarOA0efLkLV0HAADAVmuzgtPLFixYkEWLFiVJ3vWud2XffffdIkUBAABsTTZrcYjly5fnAx/4QPbff/+cdtppOe200zJkyJB88IMfzIoVK173/qZNm5Z+/fqlU6dOGT58eObPn79J42655ZZUVFS4dBAAAHhTbVZw+uxnP5vnn38+jz76aFatWpVVq1blkUceyZo1a3Laaae9rn3NnDkzEyZMyOTJk7Nw4cIMGjQoI0eOzPLly19z3JNPPpkzzzwzBx100OYcAgAAwCbbrOA0e/bsXHPNNRkwYEBz28CBAzNt2rT85Cc/eV37mjJlSsaNG5exY8dm4MCBmT59ejp37pwZM2ZsdExjY2OOP/74XHDBBdl999035xAAAAA22WYFp6ampnTo0GGD9g4dOqSpqWmT97N+/fosWLAgdXV1/yqosjJ1dXWZN2/eRsddeOGF2WWXXXLSSScVPsa6deuyZs2aFhsAAMDrsVnB6QMf+EBOP/30/PWvf21ue/rpp/O5z30uH/zgBzd5PytXrkxjY2NqampatNfU1KShoaHVMb/4xS9y/fXX57rrrtukx6ivr0+PHj2at759+25yfQAAAMlmBqevfe1rWbNmTfr165c99tgje+yxR3bbbbesWbMmX/3qV7d0jc2ef/75nHDCCbnuuuvSq1evTRozceLErF69unlbsmTJm1YfAADw1rRZy5H37ds3CxcuzD333JPf//73SZIBAwa0uORuU/Tq1StVVVVZtmxZi/Zly5altrZ2g/5//OMf8+STT+bII49sbnv50sDtttsujz32WPbYY48WY6qrq1NdXf266gIAAHilzf4ep4qKinzoQx/Khz70oc1+8I4dO2bIkCGZM2dO85LiTU1NmTNnTsaPH79B/3e+85357W9/26LtvPPOy/PPP5+rr77aZXgAAMCbYrOC02mnnZb+/ftvsPT41772tTzxxBOZOnXqJu9rwoQJGTNmTIYOHZphw4Zl6tSpWbt2bcaOHZskGT16dPr06ZP6+vp06tQp7373u1uM32GHHZJkg3YAAIAtZbM+43TrrbfmwAMP3KD9gAMOyPe///3Xta9Ro0bliiuuyKRJkzJ48OA89NBDmT17dvOCEYsXL87SpUs3p0wAAIAtYrPOOD3zzDPp0aPHBu3du3fPypUrX/f+xo8f3+qleUkyd+7c1xx74403vu7HAwAAeD0264xT//79M3v27A3af/KTn/hCWgAA4C1ns844TZgwIePHj8+KFSvygQ98IEkyZ86cXHHFFbn66qu3aIEAAADltlnB6b/+67+ybt26XHLJJbnooouSJLvttlumT5+e0aNHb9ECAQAAym2zLtX729/+ljFjxuQvf/lLli1blocffjjjx49vXtABAADgrWSzgtNRRx2Vm266KUnSoUOH1NXVZcqUKTn66KNz7bXXbtECAQAAym2zgtPChQtz0EEHJUm+//3vp6amJk899VRuuummfOUrX9miBQIAAJTbZgWnF198Md26dUuS3HXXXfnIRz6SysrKvOc978lTTz21RQsEAAAot81ejvwHP/hBlixZkjvvvDOHHnpokmT58uXp3r37Fi0QAACg3DYrOE2aNClnnnlm+vXrl+HDh2fEiBFJXjr7tO+++27RAgEAAMpts5YjP+aYY/Le9743S5cuzaBBg5rbP/jBD+bDH/7wFisOAABga7BZwSlJamtrU1tb26Jt2LBhb7ggAACArc1mXaoHAADQnghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACW0VwmjZtWvr165dOnTpl+PDhmT9//kb7zpo1K0OHDs0OO+yQLl26ZPDgwfn2t7/dhtUCAADtTdmD08yZMzNhwoRMnjw5CxcuzKBBgzJy5MgsX7681f49e/bMF77whcybNy8PP/xwxo4dm7Fjx+bOO+9s48oBAID2ouzBacqUKRk3blzGjh2bgQMHZvr06encuXNmzJjRav9DDjkkH/7whzNgwIDsscceOf3007PPPvvkF7/4RRtXDgAAtBdlDU7r16/PggULUldX19xWWVmZurq6zJs3r3B8qVTKnDlz8thjj+V973tfq33WrVuXNWvWtNgAAABej7IGp5UrV6axsTE1NTUt2mtqatLQ0LDRcatXr07Xrl3TsWPHHHHEEfnqV7+aD33oQ632ra+vT48ePZq3vn37btFjAAAA3vrKfqne5ujWrVseeuihPPjgg7nkkksyYcKEzJ07t9W+EydOzOrVq5u3JUuWtG2xAADANm+7cj54r169UlVVlWXLlrVoX7ZsWWprazc6rrKyMv3790+SDB48OIsWLUp9fX0OOeSQDfpWV1enurp6i9YNAAC0L2U949SxY8cMGTIkc+bMaW5ramrKnDlzMmLEiE3eT1NTU9atW/dmlAgAAFDeM05JMmHChIwZMyZDhw7NsGHDMnXq1KxduzZjx45NkowePTp9+vRJfX19kpc+szR06NDsscceWbduXe644458+9vfzrXXXlvOwwAAAN7Cyh6cRo0alRUrVmTSpElpaGjI4MGDM3v27OYFIxYvXpzKyn+dGFu7dm1OOeWU/OUvf8n222+fd77znbn55pszatSoch0CAADwFlf24JQk48ePz/jx41u979WLPlx88cW5+OKL26AqAACAl2yTq+oBAAC0JcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACmwVwWnatGnp169fOnXqlOHDh2f+/Pkb7XvdddfloIMOyo477pgdd9wxdXV1r9kfAADgjSp7cJo5c2YmTJiQyZMnZ+HChRk0aFBGjhyZ5cuXt9p/7ty5Oe6443Lvvfdm3rx56du3bw499NA8/fTTbVw5AADQXpQ9OE2ZMiXjxo3L2LFjM3DgwEyfPj2dO3fOjBkzWu3/P//zPznllFMyePDgvPOd78w3v/nNNDU1Zc6cOW1cOQAA0F6UNTitX78+CxYsSF1dXXNbZWVl6urqMm/evE3ax4svvph//OMf6dmzZ6v3r1u3LmvWrGmxAQAAvB5lDU4rV65MY2NjampqWrTX1NSkoaFhk/Zx9tlnZ9ddd20Rvl6pvr4+PXr0aN769u37husGAADal7JfqvdGXHbZZbnlllvyf//v/02nTp1a7TNx4sSsXr26eVuyZEkbVwkAAGzrtivng/fq1StVVVVZtmxZi/Zly5altrb2NcdeccUVueyyy3LPPfdkn3322Wi/6urqVFdXb5F6AQCA9qmsZ5w6duyYIUOGtFjY4eWFHkaMGLHRcV/+8pdz0UUXZfbs2Rk6dGhblAoAALRjZT3jlCQTJkzImDFjMnTo0AwbNixTp07N2rVrM3bs2CTJ6NGj06dPn9TX1ydJvvSlL2XSpEn5zne+k379+jV/Fqpr167p2rVr2Y4DAAB46yp7cBo1alRWrFiRSZMmpaGhIYMHD87s2bObF4xYvHhxKiv/dWLs2muvzfr163PMMce02M/kyZNz/vnnt2XpAABAO1H24JQk48ePz/jx41u9b+7cuS1uP/nkk29+QQAAAK+wTa+qBwAA0BYEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAAChQ9uA0bdq09OvXL506dcrw4cMzf/78jfZ99NFH89GPfjT9+vVLRUVFpk6d2naFAgAA7VZZg9PMmTMzYcKETJ48OQsXLsygQYMycuTILF++vNX+L774Ynbfffdcdtllqa2tbeNqAQCA9qqswWnKlCkZN25cxo4dm4EDB2b69Onp3LlzZsyY0Wr//fffP5dffnn+8z//M9XV1W1cLQAA0F6VLTitX78+CxYsSF1d3b+KqaxMXV1d5s2bt8UeZ926dVmzZk2LDQAA4PUoW3BauXJlGhsbU1NT06K9pqYmDQ0NW+xx6uvr06NHj+atb9++W2zfAABA+1D2xSHebBMnTszq1aubtyVLlpS7JAAAYBuzXbkeuFevXqmqqsqyZctatC9btmyLLvxQXV3t81AAAMAbUrYzTh07dsyQIUMyZ86c5rampqbMmTMnI0aMKFdZAAAAGyjbGackmTBhQsaMGZOhQ4dm2LBhmTp1atauXZuxY8cmSUaPHp0+ffqkvr4+yUsLSvzud79r/vfTTz+dhx56KF27dk3//v3LdhwAAMBbW1mD06hRo7JixYpMmjQpDQ0NGTx4cGbPnt28YMTixYtTWfmvk2J//etfs++++zbfvuKKK3LFFVfk4IMPzty5c9u6fAAAoJ0oa3BKkvHjx2f8+PGt3vfqMNSvX7+USqU2qAoAAOBf3vKr6gEAALxRghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAFBCcAAIACghMAAEABwQkAAKCA4AQAAFBAcAIAACggOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAgAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAU2CqC07Rp09KvX7906tQpw4cPz/z581+z//e+9728853vTKdOnbL33nvnjjvuaKNKAQCA9qjswWnmzJmZMGFCJk+enIULF2bQoEEZOXJkli9f3mr/Bx54IMcdd1xOOumk/PrXv87RRx+do48+Oo888kgbVw4AALQXZQ9OU6ZMybhx4zJ27NgMHDgw06dPT+fOnTNjxoxW+1999dU57LDDctZZZ2XAgAG56KKLst9+++VrX/taG1cOAAC0F9uV88HXr1+fBQsWZOLEic1tlZWVqaury7x581odM2/evEyYMKFF28iRI/ODH/yg1f7r1q3LunXrmm+vXr06SbJmzZrNrrtx3d82eyzbnjcyV94oc619MddoK+YabcVco61s7lx7eVypVCrsW9bgtHLlyjQ2NqampqZFe01NTX7/+9+3OqahoaHV/g0NDa32r6+vzwUXXLBBe9++fTezatqbHl/9dLlLoJ0w12gr5hptxVyjrbzRufb888+nR48er9mnrMGpLUycOLHFGaqmpqasWrUqO+20UyoqKspY2bZlzZo16du3b5YsWZLu3buXuxzewsw12oq5Rlsx12gr5trrVyqV8vzzz2fXXXct7FvW4NSrV69UVVVl2bJlLdqXLVuW2traVsfU1ta+rv7V1dWprq5u0bbDDjtsftHtXPfu3f0h0ibMNdqKuUZbMddoK+ba61N0pullZV0comPHjhkyZEjmzJnT3NbU1JQ5c+ZkxIgRrY4ZMWJEi/5Jcvfdd2+0PwAAwBtV9kv1JkyYkDFjxmTo0KEZNmxYpk6dmrVr12bs2LFJktGjR6dPnz6pr69Pkpx++uk5+OCDc+WVV+aII47ILbfckl/96lf5xje+Uc7DAAAA3sLKHpxGjRqVFStWZNKkSWloaMjgwYMze/bs5gUgFi9enMrKf50YO+CAA/Kd73wn5513Xs4999zsueee+cEPfpB3v/vd5TqEdqG6ujqTJ0/e4LJH2NLMNdqKuUZbMddoK+bam6uitClr7wEAALRjZf8CXAAAgK2d4AQAAFBAcAIAACggOAFlMXfu3FRUVOS5557b7H0ccsghOeOMM5pv9+vXL1OnTn3DtQHtT1v9//Hq/7fYtmyL8+SN7uvVz9c33nhju/1OVMFpG3DiiSemoqIiFRUV6dChQ3bbbbf893//d/7+97+3aR39+vVrrqOqqiq77rprTjrppDz77LPNfV7+42pta2hoSJKcf/75LfbTt2/fnHzyyVm1atVrjn95mzt3bpse97Zia5wn22+/ffr165ePfexj+elPf9qi3wEHHJClS5du0pfObSxkzZo1KxdddNEm1WLOvrm2xvnXuXPn7L333vnmN7/5uvZx4403tvj9de3aNUOGDMmsWbNa9DvkkENa/X1/+tOfbu7zyvbu3btn//33z2233faa41/eDjnkkDf883grKvp7O//88zdrvw8++GBOPvnkLVvsZjD/toz2NE+qqqqy4447Zvjw4bnwwguzevXqFn2LnitfqbWQVfR83Z7mbNmXI2fTHHbYYbnhhhvyj3/8IwsWLMiYMWNSUVGRL33pS21ax4UXXphx48alsbExjz/+eE4++eScdtpp+fa3v92i32OPPbbBN1bvsssuzf9+17velXvuuSeNjY1ZtGhR/uu//iurV6/Ot7/97SxdurS53+mnn541a9bkhhtuaG7r2bPnm3R0276tbZ6sX78+Tz75ZG6++ebU1dXloosuyhe+8IUkL30Bdm1t7Rt6nE2ZC+Zs29na5t+LL76Y733vexk3blz69OmTf//3f9/kfXTv3j2PPfZYkuT555/PDTfckI997GN59NFHs9deezX3GzduXC688MIWYzt37tzi9g033JDDDjssa9asyTXXXJNjjjkmCxcuzKxZs7J+/fokyZIlSzJs2LDcc889ede73pXkpb8RNvTKv7eZM2dm0qRJzb+rJOnatWvzv0ulUhobG7PddsUvd3beeectW+gbYP69ce1pnpRKpTz33HN54IEHUl9fnxtuuCH3339/dt111yRv/DloU56v28ucdcZpG1FdXZ3a2tr07ds3Rx99dOrq6nL33Xc33//MM8/kuOOOS58+fZrfZf3f//3f5vt//OMfZ4cddkhjY2OS5KGHHkpFRUXOOeec5j6f/OQn84lPfOI16+jWrVtqa2vTp0+fvP/978+YMWOycOHCDfrtsssuqa2tbbG98vu4tttuu+b91NXV5dhjj83dd9/d/Mf58rb99ts3H/vL27b+n/mbaWubJ//2b/+W973vffnGN76RL37xiy2euF59Fumpp57KkUcemR133DFdunTJu971rtxxxx158skn8/73vz9JsuOOO6aioiInnnhikk27/MCcbTtb2/zbfffdc/bZZ6dnz54t6li8eHGOOuqodO3aNd27d8/HPvaxLFu2rMU+Kioqmn9/e+65Zy6++OJUVlbm4YcfbtGvc+fOG8ybVwfwHXbYIbW1tXnHO96Riy66KP/85z9z7733pmfPns1jXn4xttNOOzXfPuuss7Lbbrtl++23z1577ZWrr756U34Nb3mv/Fn36NGjxe/q97//fbp165af/OQnGTJkSKqrq/OLX/wif/zjH3PUUUelpqYmXbt2zf7775977rmnxX5ffQlWRUVFvvnNb+bDH/5wOnfunD333DM//OEPW4x55JFH8u///u/p2rVrampqcsIJJ2TlypXN969duzajR49O165d07t371x55ZWbdIzlnn+1tbXb/Bs+7Wme9O7dOwMGDMhJJ52UBx54IC+88EL++7//u7nfq58rr7nmmuy5557p1KlTampqcswxxyR56cqB++67L1dffXXzWZwnn3xyky6tby9zVnDaBj3yyCN54IEHWrwY+/vf/54hQ4bk9ttvzyOPPJKTTz45J5xwQubPn58kOeigg/L888/n17/+dZLkvvvuS69evVpcQnTfffe9rtOcTz/9dH70ox9l+PDhb+h4nnzyydx5553t5sVlW9la5snLTj/99JRKpeZT7q926qmnZt26dfnZz36W3/72t/nSl76Url27pm/fvrn11luTvHRWaOnSpZv9AtKcbTtbw/xramrKrbfemmeffba5jqamphx11FFZtWpV7rvvvtx9993505/+lFGjRm10P42NjfnWt76VJNlvv/1ez4+hhX/+85+5/vrrkxS/M9rU1JS3ve1t+d73vpff/e53mTRpUs4999x897vf3ezHb0/OOeecXHbZZVm0aFH22WefvPDCCzn88MMzZ86c/PrXv85hhx2WI488MosXL37N/VxwwQX52Mc+locffjiHH354jj/++KxatSpJ8txzz+UDH/hA9t133/zqV7/K7Nmzs2zZsnzsYx9rHn/WWWflvvvuy2233Za77rorc+fObfWNm9dSjvnXXryV5snLdtlllxx//PH54Q9/2Pwm1Cv96le/ymmnnZYLL7wwjz32WGbPnp33ve99SZKrr746I0aMyLhx47J06dIsXbo0ffv2fd01vKXnbImt3pgxY0pVVVWlLl26lKqrq0tJSpWVlaXvf//7rznuiCOOKH3+859vvr3ffvuVLr/88lKpVCodffTRpUsuuaTUsWPH0vPPP1/6y1/+UkpSevzxxze6v7e//e2ljh07lrp06VLq1KlTKUlp+PDhpWeffba5z7333ltKUurSpUuLbeDAgc19Jk+eXKqsrGyxnySlKVOmtHrsRx111Cb+pNq3rWmeXHXVVa3eV1NTU/rMZz5TKpX+NVdenj9777136fzzz2913Kv7vuzggw8unX766Rt9bHO27WxN8+/l3/l2221XSlLq2bNn6Q9/+EOpVCqV7rrrrlJVVVVp8eLFzWMeffTRUpLS/PnzS6VSqXTDDTe0mBOVlZWl6urq0g033NDisQ4++OBShw4dNpg7N998c3OfJKVOnTo17ydJqV+/fqVnnnmmxb7+/Oc/l5KUfv3rX2/02E499dTSRz/60df8ebY3N9xwQ6lHjx7Nt1/+e/7BD35QOPZd73pX6atf/Wrz7Vf//5GkdN555zXffuGFF0pJSj/5yU9KpVKpdNFFF5UOPfTQFvtcsmRJKUnpscceKz3//POljh07lr773e823//MM8+Utt9++xb/b7V2TFvj/NuWvVXnySuP6ZWuvfbaUpLSsmXLSqVSy+fKW2+9tdS9e/fSmjVrWh376ufVUmnD5+BXP3Z7mrM+47SNeP/7359rr702a9euzVVXXZXtttsuH/3oR5vvb2xszKWXXprvfve7efrpp7N+/fqsW7euxXWjBx98cObOnZvPf/7z+fnPf576+vp897vfzS9+8YusWrUqu+66a/bcc8/XrOOss87KiSeemFKplCVLluTcc8/NEUcckZ/97Gepqqpq7vfzn/883bp1a77doUOHFvvZa6+98sMf/jB///vfc/PNN+ehhx7KZz/72Tf6Y2r3tpZ5sjGlUikVFRWt3nfaaaflM5/5TO66667U1dXlox/9aPbZZ5/NepxXMmfbztYy/17+nS9dujRnnXVWTjnllPTv3z9JsmjRovTt27fFu6gDBw7MDjvskEWLFmX//fdP8tLlfi+/4/viiy/mnnvuyac//enstNNOOfLII5vHHn/88c2f23tZTU1Ni9tXXXVV6urq8qc//Smf+9zn8pWvfGWTLimZNm1aZsyYkcWLF+dvf/tb1q9fn8GDBxeOIxk6dGiL2y+88ELOP//83H777Vm6dGn++c9/5m9/+1vhmYRX/h/UpUuXdO/ePcuXL0+S/OY3v8m9997b4rMyL/vjH//Y/Dt75Rnunj17tvi8x8ZsDfOvPdjW58nGvJQ/0urz7Yc+9KG8/e1vz+67757DDjsshx12WPNlhm9Ee5mzgtM2okuXLs1P/DNmzMigQYNy/fXX56STTkqSXH755bn66qszderU7L333unSpUvOOOOM5g/RJS9d4zpjxoz85je/SYcOHfLOd74zhxxySObOnZtnn302Bx98cGEdvXr1aq5jzz33zNSpUzNixIjce++9qaura+632267veZSlR07dmzez2WXXZYjjjgiF1xwwSav+kLrtpZ50ppnnnkmK1asyG677dbq/Z/85CczcuTI3H777bnrrrtSX1+fK6+88g2HE3O27Wwt8+/l33n//v3zve99L3vvvXeGDh2agQMHbvKxVFZWNh9L8tILo7vuuitf+tKXWrwI6NGjR4t+ramtrW2u54Ybbsjhhx+e3/3udy0WH3m1W265JWeeeWauvPLKjBgxIt26dcvll1+eX/7yl5t8DO1Zly5dWtw+88wzc/fdd+eKK65I//79s/322+eYY45pMfda8+o3UCoqKtLU1JTkpRfZRx55ZKuLn/Tu3TtPPPHEZtdf7vnXXmzr82RjFi1alO7du2ennXba4L6XA87cuXNz1113ZdKkSTn//PPz4IMPvqElxtvLnPUZp21QZWVlzj333Jx33nn529/+liS5//77c9RRR+UTn/hEBg0alN133z2PP/54i3Evf37gqquuan7x8fILkrlz527W51Zefsf+5To213nnnZcrrrgif/3rX9/QfviXrWmeJC9dO11ZWZmjjz56o3369u2bT3/605k1a1Y+//nP57rrrkvyr2ubW7te+/UyZ9vG1jL/+vbtm1GjRmXixIlJkgEDBmTJkiVZsmRJc5/f/e53ee655wqDVVVV1RueN8OGDcuQIUNyySWXvGa/+++/PwcccEBOOeWU7Lvvvunfv3/++Mc/vqHHbs/uv//+nHjiifnwhz+cvffeO7W1tXnyySff0D7322+/PProo+nXr1/zi7yXty5dumSPPfZIhw4dWoTdZ599doM5v6nacv61V2+FebJ8+fJ85zvfydFHH91igaNX2m677VJXV5cvf/nLefjhh/Pkk082f2VIx44dt8hzbfLWnLOC0zbq2GOPTVVVVaZNm5bkpXfS77777jzwwANZtGhRPvWpT22wStSOO+6YffbZJ//zP//T/OLjfe97XxYuXJjHH398k97Jff7559PQ0JClS5dm/vz5Oeuss7LzzjvngAMOaNFv+fLlaWhoaLH94x//2Oh+R4wYkX322SeXXnrp6/xJ8FrKPU+WLFmSn/3sZzn55JNz8cUX55JLLtnoO01nnHFG7rzzzvz5z3/OwoULc++992bAgAFJkre//e2pqKjIj3/846xYsSIvvPDCJv8MzNnyKdf8e7XTTz89P/rRj/KrX/0qdXV12XvvvXP88cdn4cKFmT9/fkaPHp2DDz64xWU7pVKpeR78+c9/zje+8Y3ceeedOeqoo1rs+8UXX9xg3rzye8Jac8YZZ+TrX/96nn766Y322XPPPfOrX/0qd955Zx5//PF88YtfzIMPPvi6j52X7Lnnnpk1a1Yeeuih/OY3v8nHP/7x5jMCm+vUU0/NqlWrctxxx+XBBx/MH//4x9x5550ZO3ZsGhsb07Vr15x00kk566yz8tOf/jSPPPJITjzxxI2+mH2lcs+/9mpbnSdLly7NokWLMmPGjBxwwAHp0aNHLrvsslbH/PjHP85XvvKVPPTQQ3nqqady0003pampqfnSwH79+uWXv/xlnnzyyaxcuXKTj7+9zFnBaRu13XbbZfz48fnyl7+ctWvX5rzzzst+++2XkSNH5pBDDkltbW2r7+wffPDBaWxsbH5B0rNnzwwcODC1tbWbdD3tpEmT0rt37+y66675j//4j3Tp0iV33XXXBqeD99prr/Tu3bvFtmDBgtfc9+c+97l885vfbPFOMG9MuedJ//79c8IJJ2T16tWZM2dOzj777I2OaWxszKmnnpoBAwbksMMOyzve8Y5cc801SZI+ffrkggsuyDnnnJOampqMHz9+k38G5mz5lGv+vdrAgQNz6KGHZtKkSamoqMhtt92WHXfcMe973/tSV1eX3XffPTNnzmwxZs2aNc3zYMCAAbnyyitz4YUXbnBt/nXXXbfBvDnuuONes57DDjssu+2222u+g/qpT30qH/nIRzJq1KgMHz48zzzzTE455ZTXfey8ZMqUKdlxxx1zwAEH5Mgjj8zIkSPf0GpfSbLrrrvm/vvvT2NjYw499NDsvffeOeOMM7LDDjs0v+i9/PLLc9BBB+XII49MXV1d3vve92bIkCGF+y73/GuvttV50qdPn4wYMSJf//rXM2bMmPz6179O7969Wx2zww47ZNasWfnABz6QAQMGZPr06fnf//3f5u9COvPMM1NVVZWBAwdm5513Lvx816treavP2YrSy58gAwAAoFXOOAEAABQQnAAAAAoITgAAAAUEJwAAgAKCEwAAQAHBCQAAoIDgBAAAUEBwAoBXOOSQQ3LGGWdscv8bb7wxO+yww5tWDwBbB8EJAACggOAEAABQQHACYJtwyCGH5LOf/WzOOOOM7Ljjjqmpqcl1112XtWvXZuzYsenWrVv69++fn/zkJ81j7rvvvgwbNizV1dXp3bt3zjnnnPzzn/9svn/t2rUZPXp0unbtmt69e+fKK6/c4HHXrVuXM888M3369EmXLl0yfPjwzJ07ty0OGYCtiOAEwDbjW9/6Vnr16pX58+fns5/9bD7zmc/k2GOPzQEHHJCFCxfm0EMPzQknnJAXX3wxTz/9dA4//PDsv//++c1vfpNrr702119/fS6++OLm/Z111lm57777ctttt+Wuu+7K3Llzs3DhwhaPOX78+MybNy+33HJLHn744Rx77LE57LDD8oc//KGtDx+AMqoolUqlchcBAEUOOeSQNDY25uc//3mSpLGxMT169MhHPvKR3HTTTUmShoaG9O7dO/PmzcuPfvSj3HrrrVm0aFEqKiqSJNdcc03OPvvsrF69Oi+++GJ22mmn3HzzzTn22GOTJKtWrcrb3va2nHzyyZk6dWoWL16c3XffPYsXL86uu+7aXEtdXV2GDRuWSy+9NDfeeGPOOOOMPPfcc237AwGgTW1X7gIAYFPts88+zf+uqqrKTjvtlL333ru5raamJkmyfPnyLFq0KCNGjGgOTUly4IEH5oUXXshf/vKXPPvss1m/fn2GDx/efH/Pnj2z1157Nd/+7W9/m8bGxrzjHe9oUce6deuy0047bfHjA2DrJTgBsM3o0KFDi9sVFRUt2l4OSU1NTVvk8V544YVUVVVlwYIFqaqqanFf165dt8hjALBtEJwAeEsaMGBAbr311pRKpeZAdf/996dbt25529velp49e6ZDhw755S9/mX/7t39Lkjz77LN5/PHHc/DBBydJ9t133zQ2Nmb58uU56KCDynYsAJSfxSEAeEs65ZRTsmTJknz2s5/N73//+9x2222ZPHlyJkyYkMrKynTt2jUnnXRSzjrrrPz0pz/NI488khNPPDGVlf96anzHO96R448/PqNHj86sWbPy5z//OfPnz099fX1uv/32Mh4dAG3NGScA3pL69OmTO+64I2eddVYGDRqUnj175qSTTsp5553X3Ofyyy/PCy+8kCOPPDLdunXL5z//+axevbrFfm644YZcfPHF+fznP5+nn346vXr1ynve8578x3/8R1sfEgBlZFU9AACAAi7VAwAAKCA4AQAAFBCcAAAACghOAAAABQQnAACAAoITAABAAcEJAACggOAEAABQQHACAAAoIDgBAAAUEJwAAAAKCE4AAAAF/j+zk3v0vElQsAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"model\", y=\"score\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for static data:\n",
    "# BERT: 36.72ms\n",
    "# DistilBERT: 17.83ms\n",
    "# RoBERTa: 35.59ms\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model\", \"execution_time\"])\n",
    "df.loc[len(df)] = [\"BERT\", 36.72]\n",
    "df.loc[len(df)] = [\"DistilBERT\", 17.83]\n",
    "df.loc[len(df)] = [\"RoBERTa\", 35.59]\n",
    "sns.barplot(x=\"model\", y=\"execution_time\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To our surprise, the pretrained models achieved better scores than the fine-tuned ones. We analyzed this behaviour further and noted down our observations in the report. As to the scores between model architectures, there is no significant difference when it comes to the pretrained architectures. This, combined with the fact that DistilBERT executes twice as fast, makes it the best candidate for the use in the pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
